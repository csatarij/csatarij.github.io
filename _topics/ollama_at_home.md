---
slug: ollama
title: ollama @ Home
updated: 2026-02-18
tags: [ai, learning]
---

My experiences running Ollama at home for coding.

## Network accessibility

By default, the Ollama server is only available on localhost.

## GPU computing

(NVIDIA driver + nvidia-smi)

## Context window

(Config, OpenCode)

## Tool usage experience

(GLM-47-flash)