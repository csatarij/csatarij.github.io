---
slug: ai-102
title: AI 102 Learning Journal
updated: 2026-01-15
tags: [ai, learning]
---

# AI-102 Learning Journal

## Develop generative AI apps in Azure

### Responsible AI principles

#### Fairness

AI systems should treat all people fairly. For example, suppose you create a machine learning model to support a loan approval application for a bank. The model should make predictions of whether or not the loan should be approved without incorporating any bias based on gender, ethnicity, or other factors that might result in an unfair advantage or disadvantage to specific groups of applicants.
Fairness of machine learned systems is a highly active area of ongoing research, and some software solutions exist for evaluating, quantifying, and mitigating unfairness in machine learned models. However, tooling alone isn't sufficient to ensure fairness. Consider fairness from the beginning of the application development process; carefully reviewing training data to ensure it's representative of all potentially affected subjects, and evaluating predictive performance for subsections of your user population throughout the development lifecycle.

#### Reliability and safety

AI systems should perform reliably and safely. For example, consider an AI-based software system for an autonomous vehicle; or a machine learning model that diagnoses patient symptoms and recommends prescriptions. Unreliability in these kinds of system can result in substantial risk to human life.
As with any software, AI-based software application development must be subjected to rigorous testing and deployment management processes to ensure that they work as expected before release. Additionally, software engineers need to take into account the probabilistic nature of machine learning models, and apply appropriate thresholds when evaluating confidence scores for predictions.

#### Privacy and security

AI systems should be secure and respect privacy. The machine learning models on which AI systems are based rely on large volumes of data, which may contain personal details that must be kept private. Even after models are trained and the system is in production, they use new data to make predictions or take action that may be subject to privacy or security concerns; so appropriate safeguards to protect data and customer content must be implemented.

#### Inclusiveness

AI systems should empower everyone and engage people. AI should bring benefits to all parts of society, regardless of physical ability, gender, sexual orientation, ethnicity, or other factors.
One way to optimize for inclusiveness is to ensure that the design, development, and testing of your application includes input from as diverse a group of people as possible.

#### Transparency

AI systems should be understandable. Users should be made fully aware of the purpose of the system, how it works, and what limitations may be expected.
For example, when an AI system is based on a machine learning model, you should generally make users aware of factors that may affect the accuracy of its predictions, such as the number of cases used to train the model, or the specific features that have the most influence over its predictions. You should also share information about the confidence score for predictions.
When an AI application relies on personal data, such as a facial recognition system that takes images of people to recognize them; you should make it clear to the user how their data is used and retained, and who has access to it.

#### Accountability

People should be accountable for AI systems. Although many AI systems seem to operate autonomously, ultimately it's the responsibility of the developers who trained and validated the models they use, and defined the logic that bases decisions on model predictions to ensure that the overall system meets responsibility requirements. To help meet this goal, designers and developers of AI-based solution should work within a framework of governance and organizational principles that ensure the solution meets responsible and legal standards that are clearly defined.

### Choosing between large and small language models

- LLMs like GPT-4, Mistral Large, Llama3 70B, Llama 405B, and Command R+ are powerful AI models designed for tasks that require deep reasoning, complex content generation, and extensive context understanding.
- SLMs like Phi3, Mistral OSS models, and Llama3 8B are efficient and cost-effective, while still handling many common Natural Language Processing (NLP) tasks. They're perfect for running on lower-end hardware or edge devices, where cost and speed are more important than model complexity.

### Selecting the best model for the use case

- Task type: What type of task do you need the model to perform? Does it include the understanding of only text, or also audio, or video, or multiple modalities?
- Precision: Is the base model good enough or do you need a fine-tuned model that is trained on a specific skill or dataset?
- Openness: Do you want to be able to fine-tune the model yourself?
- Deployment: Do you want to deploy the model locally, on a serverless endpoint, or do you want to manage the deployment infrastructure?

### Open versus proprietary models

- Proprietary models are best for cutting-edge performance and enterprise use. Azure offers models like OpenAI’s GPT-4, Mistral Large, and Cohere Command R+, which deliver industry-leading AI capabilities. These models are ideal for businesses needing enterprise-level security, support, and high accuracy.
- Open-source models are best for flexibility and cost-efficiency. There are hundreds of open-source models available in the Microsoft Foundry model catalog from Hugging Face, and models from Meta, Databricks, Snowflake, and Nvidia. Open models give developers more control, allowing fine-tuning, customization, and local deployment.

### Model precision evaluation

Precision: the accuracy of the model in generating correct and relevant outputs. It measures the proportion of true positive results (correct outputs) among all generated outputs. High precision means fewer irrelevant or incorrect results, making the model more reliable.

### Model performance evaluation

| Benchmark       | Description |
|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Accuracy (0/1)  | Compares model-generated text with the correct answer according to the dataset. Result is one if generated text matches the answer exactly, and zero otherwise.|
| Coherence       | Measures whether the model output flows smoothly, reads naturally, and resembles human-like language.|
| Fluency         | Assesses how well the generated text adheres to grammatical rules, syntactic structures, and appropriate usage of vocabulary, resulting in linguistically correct and natural-sounding responses.|
| Groundedness    | Measures alignment between the model's generated answers and the input data.|
| GPT Similarity  | Quantifies the semantic similarity between a ground truth sentence (or document) and the prediction sentence generated by an AI model.|
| Quality index   | A comparative aggregate score between 0 and 1, with better-performing models scoring a higher value.|
| Cost            | The cost of using the model based on a price-per-token. Cost is a useful metric with which to compare quality, enabling you to determine an appropriate tradeoff for your needs.|

### Scaling for real-world workloads

- Model deployment: Where will you deploy the model for the best balance of performance and cost?
- Model monitoring and optimization: How will you monitor, evaluate, and optimize model performance?
- Prompt management: How will you orchestrate and optimize prompts to maximize the accuracy and relevance of generated responses?
- Model lifecycle: How will you manage model, data, and code updates as part of an ongoing Generative AI Operations (GenAIOps) lifecycle?

### Deploy a language model with Microsoft Foundry

| Deployment Type      | Standard Deployment                                      | Serverless Compute                                 | Managed Compute                |
|----------------------|----------------------------------------------------------|----------------------------------------------------|-------------------------------|
| Supported Models     | Microsoft Foundry models (including Azure OpenAI models and Models-as-a-service models) | Foundry Models with pay-as-you-go billing           | Open and custom models         |
| Hosting Service      | Microsoft Foundry resource                               | AI Project resource in a hub                        | AI Project resource in a hub   |
| Billing Basis        | Token-based billing                                      | Token-based billing                                 | Compute-based billing          |

### Prompt engineering

- Instruct the model to act as a persona. "You're a seasoned marketing professional that writes advertising copy for an audience of technical customers."
- Guide the model to suggest better questions. "I have to host a dinner party for four people. What should I cook? What other information do you need to help me plan a great meal for my guests?"
- Provide a template to generate output in a specific format. "What happened in the 2018 Soccer World Cup final? Format the result to show the match date, location, and the two teams competing. Then the final score, and finally any notable events that occurred during the match."
- Understand how a model reasons by asking it to reflect. (**chain-of-thought**) "You're an AI math assistant. You always explain your answers."
- Add context to improve the accuracy of the model's output. "When should I visit Edinburgh? I'm particularly interested in attending Scotland's home matches in the Six Nations rugby tournament."

### Model optimization strategies

- Retrieval Augmented Generation (RAG): A technique that involves using a data source to provide grounding context to prompts. RAG can be a useful approach when you need the model to answer questions based on a specific knowledge domain or when you need the model to consider information related to events that occurred after the training data on which the model is based.
- Fine-tuning: A technique that involves extending the training of a foundation model by providing example prompts and responses that reflect the desired output format and style.

- Optimize for context - RAG: When the model lacks contextual knowledge and you want to **maximize responses accuracy**. What the model needs to know?
- Optimize the model - fine-tuning: When you want to improve the response format, style, or speech by **maximizing consistency of behavior**. How the model needs to act?

### Exercise: compare gpt-4o and phi-4 models

System prompt: "You are an AI assistant that helps solve problems."
User prompt: "I have a fox, a chicken, and a bag of grain that I need to take over a river in a boat. I can only take one thing at a time. If I leave the chicken and the grain unattended, the chicken will eat the grain. If I leave the fox and the chicken unattended, the fox will eat the chicken. How can I get all three things across the river without anything being eaten?"
Follow-up prompt: "Explain your reasoning."

Another test (correct answer is 40!): "I have 53 socks in my drawer: 21 identical blue, 15 identical black and 17 identical red. The lights are out, and it is completely dark. How many socks must I take out to make 100 percent certain I have at least one pair of black socks?"

### Development lifecycle of a large language model (LLM) app

1. Initialization: Define the use case and design the solution.
2. Experimentation: Develop a flow and test with a small dataset.
3. Evaluation and refinement: Assess the flow with a larger dataset.
4. Production: Deploy and monitor the flow and application.

### Flows are executable workflows often consists of three parts

1. Inputs: Represent data passed into the flow. Can be different data types like strings, integers, or boolean.
2. Nodes: Represent *tools* that perform data processing, task execution, or algorithmic operations.
    - LLM tool: Enables custom prompt creation utilizing Large Language Models.
    - Python tool: Allows the execution of custom Python scripts.
    - Prompt tool: Prepares prompts as strings for complex scenarios or integration with other tools.
3. Outputs: Represent the data produced by the flow.

### Type of flows???

- Standard flow: Ideal for general LLM-based application development, offering a range of versatile tools.
- Chat flow: Designed for conversational applications, with enhanced support for chat-related functionalities.
- Evaluation flow: Focused on performance evaluation, allowing the analysis and improvement of models or applications through feedback on previous runs.

### Implementing RAG in a prompt flow

1. Append the history to the chat input to define a prompt in the form of a contextualized form of a question.
2. Look up relevant information from your data using your search index.
3. Generate the prompt context by using the retrieved data from the index to augment the question.
4. Create prompt variants by adding a system message and structuring the chat history.
5. Submit the prompt to a language model that generates a natural language response.

### Fine-tuning a chat completion model

1. Foundational model as a base
2. Set of training data that includes example prompts and responses that the model can learn from.
    1. The system message
    2. The user message
    3. The assistant's response
3. (Optional) Validation data.

Example dataset:
`{"messages": [{"role": "system", "content": "You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox."}, {"role": "user", "content": "Is Xbox better than PlayStation?"}, {"role": "assistant", "content": "I apologize, but I cannot provide personal opinions. My primary job is to assist you with any issues related to your Xbox device. Do you have any Xbox-related issues that need addressing?"}]}`

Example multi-turn chat file format with weights:
`{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the capital of France?"}, {"role": "assistant", "content": "Paris", "weight": 0}, {"role": "user", "content": "Can you be more sarcastic?"}, {"role": "assistant", "content": "Paris, as if everyone doesn't know that already.", "weight": 1}]}`

Selecting a base model:

- Model capabilities: Evaluate the capabilities of the foundation model and how well they align with your task. For example, a model like BERT is better at understanding short texts.
- Pretraining data: Consider the dataset used for pretraining the foundation model. For example, GPT-2 is trained on unfiltered content from the internet that can result in biases.
- Limitations and biases: Be aware of any limitations or biases that might be present in the foundation model.
- Language support: Explore which models offer the specific language support or multilingual capabilities that you need for your use case.

### Plan a responsible generative AI solution

1. *Map* potential harms that are relevant to your planned solution.
    1. Identify potential harms (offensive, pejorative, discriminatory, factual inaccurate content; supporting illegal or unethical behavior or practice content)
    2. Prioritize identified harms
    3. Test and verify the prioritized harms (red teaming - <https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/red-teaming>)
    4. Document and share the verified harms
2. *Measure* the presence of these harms in the outputs generated by your solution.
    1. Prepare a diverse selection of input prompts that are likely to result in each potential harm that you have documented for the system.
    2. Submit the prompts to the system and retrieve the generated output.
    3. Apply pre-defined criteria to evaluate the output and categorize it according to the level of potential harm it contains.
3. *Mitigate* the harms at multiple layers in your solution to minimize their presence and impact, and ensure transparent communication about potential risks to users.
    1. Model: selecting appropriate model for the intended solution set, fine tuning .
    2. Safety System: platform-level configurations and capabilities. Microsoft Foundry: content filters (safe, low, medium, high), categories (hate, sexual, violence, self harm). + abuse detection algorithms (automated request from a bot)
    3. System message and grounding: system input defining behavioral parameters, prompt engineering to add grounding data to input prompts, using RAG to retrieve contextual data from trusted data sources.
    4. User experience: application UI to constrain inputs to specific subjects or types, applying input and output validation.
4. *Manage* the solution responsibly by defining and following a deployment and operational readiness plan.
    1. Complete prerelease reviews: Legal, Privacy, Security, Accessibility
    2. Release and operate the solution: phased delivery plan, incident response plan, rollback plan, blocking (harmful content, specific users/apps/IPs if misuse), feedback and report/flagging issue possibility, track telemetry

### Evaluate generative AI performance

NLP metrics: F1-score (the ratio of the number of shared words between the generated and ground truth answers), BLEU, METEOR, ROUGE
Automatic and Manual testing: JSONL: Question-ExpectedResponse

## Develop AI agents on Azure

AI agents are smart applications that use **language models** to understand what you need and then **take action** to help you. They can answer questions, make decisions, and complete tasks automatically. What makes agents special is that they **remember your conversation** and can **actually do things**, not just chat with you like a typical chatbot.

### AI agent capabilities

- **Knowledge integration and reasoning**: Uses a generative model with corporate policy documentation to answer questions accurately.
- **Task automation through functions**: Executes programmatic functions to submit expense claims automatically.
- **Intelligent decision-making**: Routes expenses to appropriate approvers based on business rules and claim amounts.

### Risks

| Example Scenario                                                                 | Risk Category                        | Description                                                                                                   |
|----------------------------------------------------------------------------------|--------------------------------------|---------------------------------------------------------------------------------------------------------------|
| The agent just shared confidential salary data in a customer chat!                | **Data leakage and privacy exposure**    | The agent accessed sensitive information but lacked proper controls to prevent exposing it externally.         |
| Someone tricked the agent into revealing our database password.                   | **Prompt injection and manipulation**    | A malicious user crafted an input that overrode the agent's intended behavior.                                 |
| Our support agent is now deleting customer records—but it shouldn't have that permission! | **Unauthorized access and privilege escalation** | Weak access controls allowed the agent to perform actions beyond its intended scope.                    |
| The agent started recommending fraudulent products after we updated the training data. | **Data poisoning**                      | Someone corrupted the agent's training or contextual data, causing unsafe outputs.                             |
| A third-party plugin we integrated is now sending our data to an unknown server.  | **Supply chain vulnerabilities**         | External dependencies introduced security vulnerabilities into the agent's workflow.                           |
| The agent automatically processed a refund without verifying the request.         | **Over-reliance on autonomous actions**  | The agent executed an action without proper validation or human oversight.                                     |
| We can't figure out who accessed what data or when.                               | **Inadequate auditability and logging**  | Missing or incomplete logs make it impossible to trace agent actions or detect misuse.                         |
| Someone extracted customer information by repeatedly querying the agent.          | **Model inversion and output leakage**   | The attacker exploited model outputs to infer sensitive data from training or prompting.                       |

### Protecting agents with security best practices

- **Control access tightly**: Enforce **role-based access controls (RBAC)** and **least privilege** permissions—agents should only access what they absolutely need.
- **Validate all inputs**: Add **prompt filtering and validation** layers to catch and block injection attacks before they reach your agent.
- **Add human oversight for critical actions**: Sandbox or gate sensitive operations behind **human-in-the-loop approvals**—don't let agents make high-stakes decisions alone.
- **Track everything**: Maintain **comprehensive logging and traceability** for all agent actions—you need to know who did what, when, and why.
- **Monitor your supply chain**: Audit **third-party dependencies** and integrations regularly—external plugins and APIs can be attack vectors.
- **Keep your models healthy**: Continuously retrain and validate models to detect **data drift** or **poisoning attempts**—agent quality degrades over time without maintenance.

### Components of an agent

- **Model**: A deployed generative AI model that enables the agent to reason and generate natural language responses to prompts. You can use common OpenAI models and a selection of models from the Microsoft Foundry model catalog.
- **Knowledge**: Data sources that enable the agent to ground prompts with contextual data. Potential knowledge sources include Internet search results from Microsoft Bing, an Azure AI Search index, or your own data and documents.
- **Tools**: Programmatic functions that enable the agent to automate *actions*. Built-in tools to access knowledge in Azure AI Search and Bing are provided as well as a code interpreter tool that you can use to generate and run Python code. You can also create custom tools using your own code or Azure Functions.

### Why are agents useful?

- **Automation of Routine Tasks**: AI agents can handle repetitive and mundane tasks, freeing up human workers to focus on more strategic and creative activities. This leads to increased productivity and efficiency.
- **Enhanced Decision-Making**: By processing vast amounts of data and providing insights, AI agents support better decision-making. They can analyze trends, predict outcomes, and offer recommendations based on real-time data. AI Agents can even use advanced decision-making algorithms and machine learning models to analyze data and make informed decisions autonomously. This allows them to handle complex scenarios and provide actionable insights, whereas generative AI chat models primarily focus on generating text-based responses.
- **Scalability**: AI agents can scale operations without the need for proportional increases in human resources. This is beneficial for businesses looking to grow without significantly increasing operational costs.
- **24/7 Availability**: Like all software, AI agents can operate continuously without breaks, ensuring that tasks are completed promptly and customer service is available around the clock.

Examples of AI agent use cases:

- Personal productivity agents
- Research agents
- Sales agents
- Customer service agents
- Developer agents

### AI Tools/Actions

- Knowledge tools
  - Bing Search
  - File search
  - Azure AI Search
  - Microsoft Fabric
- Action tools
  - Code interpreter
  - Custom function: Function calling allows you to describe the structure of custom functions to an agent and return the functions that need to be called along with their arguments.
  - Azure Functions: Azure Functions enable you to create intelligent, event-driven applications with minimal overhead.
  - OpenAPI Specification tools: These tools allow you to connect your Azure AI Agent to an external API using an OpenAPI 3.0 specification.
  - Azure Logic Apps: This action provides low-code/no-code solutions to add workflows and connects apps, data, and services with the low-code Logic App.

### Develop AI agents with the Microsoft/Azure AI Foundry extension in Visual Studio Code

1. Install and configure the extension in Visual Studio Code
2. Connect to your Microsoft Foundry project
3. Create or import an AI agent using the designer
4. Configure agent instructions and add necessary tools
5. Test the agent using the integrated playground
6. Iterate on the design based on test results
7. Generate code for application integration

### Model Context Protocl (MCP) servers

MCP servers provide a standardized way to add tools to your agents using an open protocol.

- Standardized protocol for consistent tool communication
- Reusable components that work across different agents
- Community-driven tools available through MCP registries
- Simplified integration with consistent interfaces

### Designing a multi-agent solution with connected agents

Main agent responsibilities: The main agent acts as the orchestrator. It interprets the intent behind a request and determines which connected agent is best suited to handle it. The main agent is responsible for:

- Interpreting user input
- Selecting the appropriate connected agent
- Forwarding relevant context and instructions
- Aggregating or summarize results

Connected agent responsibilities: Connected agents designed to focus on a single domain of responsibility. A connected agent is responsible for:

- Completing a specific action based on a clear prompt
- Using tools (if needed) to complete their task
- Returning the results to the main agent

### Microsoft Connector Protocol (MCP)

- Dynamic Tool Discovery: AI agents can automatically receive a list of available tools from a server, along with descriptions of their functions. Unlike traditional APIs, which often require manual coding for each integration and updates whenever the API changes, MCP enables an “integrate once” approach that improves adaptability and reduces maintenance.
- Interoperability Across LLMs: MCP works seamlessly with different large language models (LLMs), allowing developers to switch or evaluate core models for improved performance without reworking integrations.
- Standardized Security: MCP provides a consistent authentication method, simplifying secure access across multiple MCP servers. This eliminates the need to manage separate keys or authentication protocols for each API, making it easier to scale AI agent deployments.

### Model Context Protocol integration

MCP Server: The MCP server acts as a registry for tools your agent can use. You can initialize your MCP server using FastMCP("server-name"). The FastMCP class uses Python type hints and document strings to automatically generate tool definitions, making it easy to create and maintain MCP tools.

MCP Client: A standard MCP client acts as a bridge between your MCP server and the Azure AI Agent Service. The client initializes an MCP client session and connects to the server. Afterwards, it performs three key tasks:

- Discovers available tools from the MCP server using session.list_tools().
- Generates Python function stubs that wrap the tools.
- Registers those functions with your agent.

#### Overview of MCP agent tool integration

1. The MCP server hosts tool definitions decorated with @mcp.tool.
2. The MCP client initializes an MCP client connection to the server.
3. The MCP client fetches the available tool definitions with session.list_tools().
4. Each tool is wrapped in an async function that invokes session.call_tool
5. The tool functions are bundled into FunctionTool that makes them usable by the agent.
6. The FunctionTool is registered to the agent's toolset.

#### Invoking tools when using the Azure MCP Tool object

- Create the McpTool object with the server label and url.
- Use update_headers to apply any headers required by the server.
- Use the set_approval_mode to determine whether approval is required. Supported values are:
  - always: A developer needs to provide approval for every call. If you don't provide a value, this one is the default.
  - never: No approval is required.
- Create a ToolSet object and add the McpTool object
- Create an agent run and specify the toolset property
- When the run completes, you should see the results of any invoked tools in the response.

#### Best practices for tool development

- **Clear descriptions**: Write clear, detailed descriptions for your functions and parameters to help the AI understand their purpose
- **Type annotations**: Use proper Python type hints to specify expected input and output types
- **Error handling**: Implement appropriate error handling in your tool functions to gracefully handle unexpected inputs
- **Return meaningful data**: Ensure your functions return data that the AI can effectively use in its responses
- **Keep functions focused**: Design each tool to handle a specific task rather than trying to do too many things in one function

### Why multi-agent orchestration matters

- Assign distinct skills, responsibilities, or perspectives to each agent.
- Combine outputs from multiple agents to improve decision-making and accuracy.
- Coordinate steps in a workflow so each agent’s work builds on the last.
- Dynamically route control between agents based on context or rules.

### Core Components of a Workflow

- Direct Edges: Connect one executor directly to another in sequence.
*Example: After an AI agent gathers user input, the next executor processes the booking.*

- Conditional Edges: Trigger only when certain conditions are met.
*Example: If hotel rooms are unavailable, the workflow branches to an executor that suggests alternative dates or locations.*

- Switch-Case Edges: Route messages to different executors based on predefined conditions.
*Example: VIP customers might be routed to a premium service executor, while others follow the standard process.*

- Fan-Out Edges: Send a single message to multiple executors simultaneously.
*Example: One request could be sent to several agents — one checking flights, another checking hotels.*

- Fan-In Edges: Combine multiple messages from different executors into one for a final step.
*Example: After gathering hotel and flight results, a summary executor compiles them into a single travel itinerary.*

### Supported orchestration patterns

- **Concurrent orchestration** - Broadcast the same task to multiple agents at once and collect their results independently. Useful for parallel analysis, independent subtasks, or ensemble decision making.
- **Sequential orchestration** - Pass the output from one agent to the next in a fixed order. Ideal for step-by-step workflows, pipelines, and progressive refinement.
- **Handoff orchestration** - Dynamically transfer control between agents based on context or rules. Great for escalation, fallback, and expert routing where one agent works at a time.
- **Group chat orchestration** - Coordinate a shared conversation among multiple agents (and optionally a human), managed by a chat manager that chooses who speaks next. Best for brainstorming, collaborative problem solving, and building consensus.
- **Magentic orchestration** - A manager-driven approach that plans, delegates, and adapts across specialized agents. Suited to complex, open-ended problems where the solution path evolves.

### Agent-to-Agent (A2A) protocol

The **Agent-to-Agent (A2A)** protocol is a standardized way for AI agents to communicate and collaborate with each other. It defines how agents can share context, invoke each other's capabilities, and exchange information securely. By adhering to the A2A protocol, agents from different vendors or platforms can work together seamlessly, enabling more complex and integrated AI solutions.

Before an A2A agent can participate in multi-agent workflows, it needs to explain what it can do. **Agent Skills** and how other agents or clients can discover those capabilities are exposed through an **Agent Card**.

Advantage over MCP: Each A2A agent can choose which large language model (LLM) to use for handling requests, **enabling optimized or fine-tuned models per agent**, unlike some MCP scenarios that rely on a single LLM connection.

The **Agent Executor** is a core component of an A2A agent. The AgentExecutor interface handles all incoming requests sent to your agent. It receives information about the request, processes it according to the agent’s capabilities, and sends responses or events back through a communication channel.

Two primary operations:

- **Execute**
  - Processes incoming requests and generates responses.
  - Accesses request details (for example, user input, task context).
  - Sends results back via an event queue, which may include messages, task updates, or artifacts.
- **Cancel**
  - Handles requests to cancel an ongoing task.
  - May not be supported for simple agents.

The executor uses the RequestContext to understand the incoming request and an EventQueue to communicate results or events back to the client.

### Build agent-driven workflows using Microsoft Foundry

Workflows in Microsoft Foundry provide a way to orchestrate AI-driven actions using a visual, declarative approach. A workflow consists of connected nodes, where each node performs a specific function. Some nodes invoke agents, while others evaluate conditions, manage data, or communicate with users. Together, these nodes form an execution path that determines how requests move through the system.

Workflow patterns:

- A **sequential workflow** follows a fixed, step-by-step path. Each node executes in order, passing its output to the next step in the workflow. This pattern works well for pipelines and multi-stage processes, such as validating input, enriching data, and generating a final response. Sequential workflows are predictable and easy to reason about, making them a good starting point when you're learning how workflows operate.

- A **human-in-the-loop workflow** introduces pauses where user input or approval is required before the workflow can continue. In this pattern, the workflow explicitly asks a question, waits for a response, and then resumes execution based on that input. Human-in-the-loop workflows are useful when automation must be balanced with oversight—such as approvals, confirmations, or situations where missing context needs to be provided by a person.

- A **group chat workflow** enables more dynamic orchestration across multiple agents. Instead of following a fixed path, control can shift between agents based on context, rules, or intermediate results. This pattern is useful for scenarios where multiple specialized agents collaborate to handle complex requests, such as customer support or multi-domain question answering. Group chat workflows allow for flexible interactions, where agents can build on each other's outputs and adapt to changing inputs.

The main node types in the workflow builder are:

- **Invoke**: Invokes an AI agent from your project or creates a new one. Agent nodes can return free-text responses or structured outputs (like JSON) that other nodes can use. They're used for classification, reasoning, recommendations, or any AI-driven task.
- **Flow**: Controls the workflow's execution path. Flow nodes let your workflow adapt dynamically to different inputs or situations. Flow nodes include:
  - If/Else: Branches execution based on conditions.
  - Go To: Jumps to another node in the workflow.
  - For Each: Loops over a list of items, performing the same actions for each one.
- **Data transformation**: Manipulates data and manages variables. Data transformation nodes ensure that information is correctly passed to subsequent steps. Data transformation nodes include:
  - Set Variable: Assigns a value to a variable for later use.
  - Reset Variable: Clears or reinitializes a variable.
  - Parse value: Extracts specific data from structured outputs or converts values to different formats.
- **Basic chat**: Sends messages to the user or asks questions to collect input. These nodes are often paired with variables to capture responses, which can then influence logic or agent decisions later in the workflow.
- **End**: Marks the conclusion of a workflow. The End node can optionally return a final result or status.

## Develop natural language solutions in Azure

### Azure Language functionalities

- Language detection - determining the language in which text is written.
- Key phrase extraction - identifying important words and phrases in the text that indicate the main points.
- Sentiment analysis - quantifying how positive or negative the text is.
- Named entity recognition - detecting references to entities, including people, locations, time periods, organizations, and more.
- Entity linking - identifying specific entities by providing reference links to Wikipedia articles.
- Summarization - available for both documents and conversations, and will summarize the text into key sentences that are predicted to encapsulate the input's meaning.
- Personally identifiable information (PII) detection - allows you to identify, categorize, and redact information that could be considered sensitive, such as email addresses, home addresses, IP addresses, names, and protected health information.

### Learned features

- Conversational language understanding (CLU): CLU helps users to build custom natural language understanding models to predict overall intent and extract important information from incoming utterances. CLU does require data to be tagged by the user to teach it how to predict intents and entities accurately.
- Custom named entity recognition: takes custom labeled data and extracts specified entities from unstructured text.
- Custom text classification: enables users to classify text or documents as custom defined groups.
- Question answering: is a mostly pre-configured feature that provides answers to questions provided as input. The data to answer these questions comes from documents like FAQs or manuals.

### Comparing question answering to Azure Language understanding

| Feature              | Question Answering                                                                 | Language Understanding                                                                                 |
|----------------------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|
| **Usage pattern**    | User submits a question, expecting an answer                                      | User submits an utterance, expecting an appropriate response or action                                |
| **Query processing** | Service uses natural language understanding to match the question to an answer in the knowledge base | Service uses natural language understanding to interpret the utterance, match it to an intent, and identify entities |
| **Response**         | Response is a static answer to a known question                                   | Response indicates the most likely intent and referenced entities                                      |
| **Client logic**     | Client application typically presents the answer to the user                      | Client application is responsible for performing appropriate action based on the detected intent       |

### Intents, utterances and entities

*Utterances* are the phrases that a user might enter when interacting with an application that uses your language model. An *intent* represents a task or action the user wants to perform, or more simply the *meaning* of an utterance. You create a model by defining intents and associating them with one or more utterances. *Entities* are used to add specific context to intents.

Guidelines for capturing example utterances for each intent:

- Capture multiple different examples, or alternative ways of saying the same thing
- Vary the length of the utterances from short, to medium, to long
- Vary the location of the noun or subject of the utterance. Place it at the beginning, the end, or somewhere in between
- Use correct grammar and incorrect grammar in different utterances to offer good training data examples
- The precision, consistency and completeness of your labeled data are key factors to determining model performance.
  - Label precisely: Label each entity to its right type always. Only include what you want extracted, avoid unnecessary data in your labels.
  - Label consistently: The same entity should have the same label across all the utterances.

| Utterance                              | Intent         | Entities                      |
|-----------------------------------------|----------------|-------------------------------|
| What is the time?                       | GetTime        |                               |
| What time is it in London?              | GetTime        | Location (London)             |
| What's the weather forecast for Paris?  | GetWeather     | Location (Paris)              |
| Will I need an umbrella tonight?        | GetWeather     | Time (tonight)                |
| What's the forecast for Seattle tomorrow?| GetWeather    | Location (Seattle), Time (tomorrow) |
| Turn the light on.                      | TurnOnDevice   | Device (light)                |
| Switch on the fan.                      | TurnOnDevice   | Device (fan)                  |

Entity component types:

- **Learned** entities are the most flexible kind of entity, and should be used in most cases. You define a learned component with a suitable name, and then associate words or phrases with it in training utterances. 
- **List** entities are useful when you need an entity with a specific set of possible values - for example, days of the week. You can include synonyms in a list entity definition, so you could define a DayOfWeek entity that includes the values "Sunday", "Monday", "Tuesday", and so on; each with synonyms like "Sun", "Mon", "Tue", and so on.
- **Prebuilt** entities are useful for common types such as numbers, datetimes, and names. For example, when prebuilt components are added, you will automatically detect values such as "6" or organizations such as "Microsoft".

Example intents with different formats of utterance:

- TurnOnDevice:
  - "Turn on the {DeviceName}"
  - "Switch on the {DeviceName}"
  - "Turn the {DeviceName} on"
- GetDeviceStatus:
  - "Is the {DeviceName} on[?]"
- TurnOffDevice:
  - "Turn the {DeviceName} off"
  - "Switch off the {DeviceName}"
  - "Turn off the {DeviceName}"

Creating a model is an iterative process with the following activities:

1. Train a model to learn intents and entities from sample utterances.
2. Test the model interactively or using a testing dataset with known labels
3. Deploy a trained model to a public endpoint so client apps can use it
4. Review predictions and iterate on utterances to train your model

### Custom text classification

Custom text classification assigns labels, which in the Azure Language service is a class that the developer defines, to text files.

- Single label classification - you can assign only one class to each file. Following the above example, a video game summary could only be classified as "Adventure" or "Strategy".
- Multiple label classification - you can assign multiple classes to each file. This type of project would allow you to classify a video game summary as "Adventure" or "Adventure and Strategy".

Evaluating the model:

False positive - model predicts x, but the file isn't labeled x.
False negative - model doesn't predict label x, but the file in fact is labeled x.

- Recall - Of all the actual labels, how many were identified; the ratio of true positives to all that was labeled.
- Precision - How many of the predicted labels are correct; the ratio of true positives to all identified positives.
- F1 Score - A function of recall and precision, intended to provide a single score to maximize for a balance of each component

Azure Language project life cycle:

- Define labels/entities: Understanding the data you want to classify, identify the possible labels you want to categorize into.
- Tag data: Tag, or label, your existing data, specifying the label or labels each file falls under.
- Train model: Train your model with the labeled data.
- View model: After your model is trained, view the results of the model. Your model is scored between 0 and 1, based on the precision and recall of the data tested. Take note of which class didn't perform well.
- Improve model: Improve your model by seeing which classifications failed to evaluate to the right label, see your label distribution, and find out what data to add to improve performance. Try to find more examples of each label to add to your dataset for retraining your model.
- Deploy model: Once your model performs as desired, deploy your model to make it available via the API.
- Classify text/extract entities: Use your model for classifying text.

Splitting the dataset:

- Training - The training dataset is used to actually train the model; the data and labels provided are fed into the machine learning algorithm to teach your model what data should be classified to which label. The training dataset will be the larger of the two datasets, recommended to be about 80% of your labeled data.
- Testing - The testing dataset is labeled data used to verify you model after it's trained. Azure will take the data in the testing dataset, submit it to the model, and compare the output to how you labeled your data to determine how well the model performed. The result of that comparison is how your model gets scored and helps you know how to improve your predictive performance.

- Automatic split - Azure takes all of your data, splits it into the specified percentages randomly, and applies them in training the model. This option is best when you have a larger dataset, data is naturally more consistent, or the distribution of your data extensively covers your classes.
- Manual split - Manually specify which files should be in each dataset. When you submit the training job, the Azure Language service will tell you the split of the dataset and the distribution. This split is best used with smaller datasets to ensure the correct distribution of classes and variation in data are present to correctly train your model.

### Custom named entity extraction

Custom NER is an Azure API service that looks at documents, identifies, and extracts user defined entities. These entities could be anything from names and addresses from bank statements to knowledge mining to improve search results.

High quality data will let you spend less time refining and yield better results from your model.

- Diversity - use as diverse of a dataset as possible without losing the real-life distribution expected in the real data. You'll want to use sample data from as many sources as possible, each with their own formats and number of entities. It's best to have your dataset represent as many different sources as possible.
- Distribution - use the appropriate distribution of document types. A more diverse dataset to train your model will help your model avoid learning incorrect relationships in the data.
- Accuracy - use data that is as close to real world data as possible. Fake data works to start the training process, but it likely will differ from real data in ways that can cause your model to not extract correctly.

Entities need to also be carefully considered, and defined as distinctly as possible. "Contact info" => "Name", "Email", "Phone"

Labeling data:

- Consistency - Label your data the same way across all files for training. Consistency allows your model to learn without any conflicting inputs.
- Precision - Label your entities consistently, without unnecessary extra words. Precision ensures only the correct data is included in your extracted entity.
- Completeness - Label your data completely, and don't miss any entities. Completeness helps your model always recognize the entities present.

### Azure Translator resource

Azure Translator provides a multilingual text translation API that you can use for:

- Language detection.
- One-to-many translation.
- Script transliteration (converting text from its native script to an alternative script).

Translation options:

1. Word alignment
  In written English (using Latin script), spaces are used to separate words. However, in some other languages (and more specifically, scripts) this is not always the case.
  For example, translating "Smart Services" from en (English) to zh (Simplified Chinese) produces the result "智能服务", and it's difficult to understand the relationship between the characters in the source text and the corresponding characters in the translation. To resolve this problem, you can specify the includeAlignment parameter with a value of true in your call.

2. Sentence length
  Sometimes it might be useful to know the length of a translation, for example to determine how best to display it in a user interface. You can get this information by setting the includeSentenceLength parameter to true.

3. Profanity filtering
  Sometimes text contains profanities, which you might want to obscure or omit altogether in a translation. You can handle profanities by specifying the profanityAction parameter, which can have one of the following values:

- NoAction: Profanities are translated along with the rest of the text.
- Deleted: Profanities are omitted in the translation.
- Marked: Profanities are indicated using the technique indicated in the profanityMarker parameter (if supplied). The default value for this parameter is Asterisk, which replaces characters in profanities with "*". As an alternative, you can specify a profanityMarker value of Tag, which causes profanities to be enclosed in XML tags.

### Azure Speech to Text API

- **Real-time transcription**: Instant transcription with intermediate results for live audio inputs.
- **Fast transcription**: Fastest synchronous output for situations with predictable latency.
- **Batch transcription**: Efficient processing for large volumes of prerecorded audio.
- **Custom speech**: Models with enhanced accuracy for specific domains and conditions.

### Azure Text to Speech API

- The **Text to speech** API, which is the primary way to perform speech synthesis.
- The **Batch synthesis** API, which is designed to support batch operations that convert large volumes of text to audio - for example to generate an audio-book from the source text.

### Speech Synthesis Markup Language

The Speech Synthesis Markup Language (SSML) syntax offers greater control over how the spoken output sounds, enabling you to:

- Specify a speaking style, such as "excited" or "cheerful" when using a neural voice.
- Insert pauses or silence.
- Specify *phonemes* (phonetic pronunciations), for example to pronounce the text "SQL" as "sequel".
- Adjust the *prosody* of the voice (affecting the pitch, timbre, and speaking rate).
- Use common "say-as" rules, for example to specify that a given string should be expressed as a date, time, telephone number, or other form.
- Insert recorded speech or audio, for example to include a standard recorded message or simulate background noise.

### Translating speech: synthesize translations

- **Event-based synthesis**: When you want to perform 1:1 translation (translating from one source language into a single target language), you can use event-based synthesis to capture the translation as an audio stream.
- **Manual synthesis**: Manual synthesis is an alternative approach to event-based synthesis that doesn't require you to implement an event handler. You can use manual synthesis to generate audio translations for one or more target languages.

### Azure AI Voice live API

The Voice live API enables developers to create voice-enabled applications with real-time, bidirectional communication.

The Voice live API provides real-time communication using WebSocket connections. It supports advanced features such as speech recognition, text-to-speech synthesis, avatar streaming, and audio processing.

- JSON-formatted events manage conversations, audio streams, and responses.
- Events are categorized into client events (sent from client to server) and server events (sent from server to client).

Key features:

- Real-time audio processing with support for multiple formats like PCM16 and G.711.
- Advanced voice options, including OpenAI voices and Azure custom voices.
- Avatar integration using WebRTC for video and animation.
- Built-in noise reduction and echo cancellation.

## Develop computer vision solutions in Azure

### Azure Vision service

- Image analysis
  - Generate a caption for an image based on its contents.
  - Suggest appropriate tags to associate with an image.
  - Detect and locate common objects in an image.
  - Detect and locate people in an image.
- Optical character recognition (OCR)
- Face detection and analysis
- Video analysis

Analyze Image REST available visual features:

- VisualFeatures.Tags: Identifies tags about the image, including objects, scenery, setting, and actions
- VisualFeatures.Objects: Returns the bounding box for each detected object
- VisualFeatures.Caption: Generates a caption of the image in natural language
- VisualFeatures.DenseCaptions: Generates more detailed captions for the objects detected
- VisualFeatures.People: Returns the bounding box for detected people
- VisualFeatures.SmartCrops: Returns the bounding box of the specified aspect ratio for the area of interest
- VisualFeatures.Read: Extracts readable text (OCR)

### Reading text options

**Azure Vision**: Azure Vision includes an image analysis capability that supports optical character recognition (OCR).

- Text location and extraction from scanned documents.
- Finding and reading text in photographs.
- Digital asset management (DAM).

**Azure Document Intelligence**: Azure Document Intelligence is a service that you can use to extract information from complex digital documents. Azure Document Intelligence is designed for extracting text, key-value pairs, tables, and structures from documents automatically and accurately.

- Form processing.
- Prebuilt models.
- Custom models.

**Azure Content Understanding**: Azure Content Understanding is a service that you can use to analyze and extract information from multiple kinds of content; including documents, images, audio streams, and video.

- Multimodal content extraction.
- Custom content analysis scenarios.

### Detect, analyze, and recognize faces

The Face service provides functionality that you can use for:

1. Face detection - for each detected face, the results include an ID that identifies the face and the bounding box coordinates indicating its location in the image.
2. Face attribute analysis - you can return a wide range of facial attributes, including:
    - Head pose (pitch, roll, and yaw orientation in 3D space)
    - Glasses (No glasses, Reading glasses, Sunglasses, or Swimming Goggles)
    - Mask (the presence of a face mask)
    - Blur (low, medium, or high)
    - Exposure (under exposure, good exposure, or over exposure)
    - Noise (visual noise in the image)
    - Occlusion (objects obscuring the face)
    - Accessories (glasses, headwear, mask)
    - QualityForRecognition (low, medium, or high)
3. Facial landmark location - coordinates for key landmarks in relation to facial features (for example, eye corners, pupils, tip of nose, and so on)
4. Face comparison - you can compare faces across multiple images for similarity (to find individuals with similar facial features) and verification (to determine that a face in one image is the same person as a face in another image)
5. Facial recognition - you can train a model with a collection of faces belonging to specific individuals, and use the model to identify those people in new images.
6. Facial liveness - liveness can be used to determine if the input video is a real stream or a fake to prevent bad-intentioned individuals from spoofing a facial recognition system.

### Azure AI Custom Vision

The Azure AI Custom Vision service enables you to build your own computer vision models for image classification or object detection.

To use the Custom Vision service to create a solution, you need two Custom Vision resources in your Azure subscription:

- An Azure AI Custom Vision training resource - used to train a custom model based on your own training images.
- An Azure AI Custom Vision prediction resource - used to generate predictions from new images based on your trained model.

There are two components to an object detection prediction:

- The class label of each object detected in the image. For example, you might ascertain that an image contains an apple, an orange, and a banana.
- The location of each object within the image, indicated as coordinates of a bounding box that encloses the object.

### Azure Video Indexer

The Azure Video Indexer service is designed to help you extract information from videos. It provides functionality that you can use for:

- Facial recognition - detecting the presence of individual people in the image.
- Optical character recognition - reading text in the video.
- Speech transcription - creating a text transcript of spoken dialog in the video.
- Topics - identification of key topics discussed in the video.
- Sentiment - analysis of how positive or negative segments within the video are.
- Labels - label tags that identify key objects or themes throughout the video.
- Content moderation - detection of adult or violent themes in the video.
- Scene segmentation - a breakdown of the video into its constituent scenes.

Extract custom insights:

Azure Video Indexer includes predefined models that can recognize well-known celebrities, do OCR, and transcribe spoken phrases into text. You can extend the recognition capabilities of Video Analyzer by creating custom models for:

- People. Add images of the faces of people you want to recognize in videos, and train a model.
- Language. If your organization uses specific terminology that may not be in common usage, you can train a custom model to detect and transcribe it.
- Brands. You can train a model to recognize specific names as brands, for example to identify products, projects, or companies that are relevant to your business.

## Develop AI information extraction solutions in Azure

### Azure Content Understanding

Azure Content Understanding is a generative AI service that you can use to extract insights and data from multiple kinds of content.

Content Understanding can extract information from common kinds of content (multimodal), enabling you to use a single service with a straightforward and consistent development process to build multimodal content analysis solutions.

- Documents and forms: You can use Content Understanding to analyze documents and forms and retrieve specific field values.
- Images: You can analyze images to infer information from visuals such as charts, identify physical defects in products or other items, detect the presence of specific objects or people, or determine other information visually.
- Audio: Analysis of audio enables you to automate tasks like summarizing conference calls, determining sentiment of recorded customer conversations, or extracting key data from telephone messages.
- Video: Video accounts for a large volume of the data captured today, and you can use Content Understanding to analyze and extract insights from video to support many scenarios. For example, to extract key points from video conference recordings, to summarize presentations, or to detect the presence of specific activity in security footage.

The high-level process for creating a Content Understanding solution includes the following steps:

1. Create a Foundry Tools resource.
2. Define a Content Understanding schema for the information to be extracted. This can be based on a content sample and an analyzer template.
3. Build an analyzer based on the completed schema.
4. Use the analyzer to extract or generate fields from new content.

### Azure Document Intelligence

Prebuilt models to extract data from one of these common forms or documents:

- Invoice model. Extracts common fields and their values from invoices.
- Receipt model. Extracts common fields and their values from receipts.
- US Tax model. Unified US tax model that can extract from forms such as W-2, 1098, 1099, and 1040.
- ID document model. Extracts common fields and their values from US drivers' licenses, European Union IDs and drivers license, and international passports.
- Business card model. Extracts common fields and their values from business cards.
- Health insurance card model. Extracts common fields and their values from health insurance cards.
- Marriage certificate. Extracts information from marriage certificates.
- Credit/Debit card model. Extracts common information from bank cards.
- Mortgage documents. Extracts information from mortgage closing disclosure, Uniform Residential Loan Application (Form 1003), Appraisal (Form 1004), Validation of Employment (Form 1005), and Uniform Underwriting and Transmittal Summary (Form 1008).
- Bank statement model. Extracts account information including beginning and ending balances, transaction details from bank statements.
- Pay Stub model. Extracts wages, hours, deductions, net pay, and other common pay stub fields.
- Check model. Extracts payee, amount, date, and other relevant information from checks.

The other models are designed to extract values from documents with less specific structures:

- Read model. Extracts printed and handwritten text from documents and images.
  - The read model can also detect the language that a line of text is written in and classify whether it's handwritten or printed text.
  - Ideal if you want to extract words and lines from documents with no fixed or predictable structure.
- General document model. Extract text, keys, values, entities, and selection marks from documents.
  - Extends the functionality of the read model by adding the detection of key-value pairs, entities, selection marks, and tables.
  - The model can extract these values from structured, semi-structured, and unstructured documents.
  - The types of entities you can detect include:
    - Person. The name of a person.
    - PersonType. A job title or role.
    - Location. Buildings, geographical features, geopolitical entities.
    - Organization. Companies, government bodies, sports clubs, musical bands, and other groups.
    - Event. Social gatherings, historical events, anniversaries.
    - Product. Objects bought and sold.
    - Skill. A capability belonging to a person.
    - Address. Mailing address for a physical location.
    - Phone number. Dialing codes and numbers for mobile phones and landlines.
    - Email. Email addresses.
    - URL. Webpage addresses.
    - IP Address. Network addresses for computer hardware.
    - DateTime. Calendar dates and times of day.
    - Quantity. Numerical measurements with their units.
- Layout model. Extracts text and structure information from documents. Returns selection marks and tables from the input image or PDF file. It's a good model to use when you need rich information about the structure of a document.

#### Decide what component of Azure Document Intelligence to use

| Use case | Recommended features to use |
|----------|-----------------------------|
| Use OCR capabilities to capture document analysis | Use the Layout model, Read model, or General Document model. |
| Create an application that extracts data from W-2s, Invoices, Receipts, ID documents, Health insurance, vaccination, and business cards | Use a prebuilt model. These models don't need to be trained. Azure Document Intelligence services analyze the documents and return a JSON output. |
| Create an application to extract data from your industry-specific forms | Use a custom model. This model needs to be trained on sample documents. After you train the custom model, it can analyze new documents and return a JSON output. |

### Azure AI Search

Azure AI Search provides a cloud-based solution for indexing and querying a wide range of data sources, and creating comprehensive and high-scale search solutions. It provides the infrastructure and tools to create search solutions that extract data from structured, semi-structured, and non-structured documents and other data sources.

With Azure AI Search, you can:

- Index documents and data from a range of sources.
- Use AI skills to enrich index data.
- Store extracted insights in a knowledge store for analysis and integration.

Azure AI search has many applications, including:

- Implementing an enterprise search solution to help employees or customers find information in websites or applications.
- Supporting retrieval augmented generation (RAG) in generative AI applications by using vector-based indexes for prompt grounding data.
- Creating knowledge mining solutions in which the indexing process is used to infer insights and extract granular data assets from documents to support data analytics.

An index contains your searchable content and is created and updated, unsurprisingly, by an indexer.

The indexing process works by creating a document for each indexed entity. During indexing, an enrichment pipeline iteratively builds the documents that combine metadata from the data source with enriched fields extracted or generated by skills.

- document
  - metadata_storage_name
  - metadata_author
  - content
  - normalized_images
    - image0
      - Text (ocr skill)
    - image1
      - Text
  - language (coming from skill)
  - merged_content (combined original text with image text)

The enrichment pipeline that is orchestrated by an indexer uses a skillset of AI skills to create AI-enriched fields. The indexer applies each skill in order, refining the index document at each step.

Azure AI Search provides a collection of **built-in skills** that you can include in a skillset for your indexer. Built-in skills include functionality from Foundry Tools such as Azure Vision and Azure Language, enabling you to apply enrichments such as:

- Detecting the language that text is written in.
- Detecting and extracting places, locations, and other entities in the text.
- Determining and extracting key phrases within a body of text.
- Translating text.
- Identifying and extracting (or removing) personally identifiable information (PII) within the text.
- Extracting text from images.
- Generating captions and tags to describe images.

**Custom skills** perform custom logic on input data from your index document to return new field values that can be incorporated into the index. Often, custom skills are "wrappers" around services that are specifically designed to extract data from documents. For example, you could implement a custom skill as an Azure Function, and use it to pass data from your index document to an Azure Document Intelligence model, which can extract fields from a form.

#### Full-text search

Full-text search describes search solutions that parse text-based document contents to find query terms. Full-text search queries in Azure AI Search are based on the Lucene query syntax, which provides a rich set of query operations for searching, filtering, and sorting data in indexes.

Client applications submit queries to Azure AI Search by specifying a search expression along with other parameters that determine how the expression is evaluated and the results returned.

Query processing consists of four stages:

1. Query parsing. The search expression is evaluated and reconstructed as a tree of appropriate subqueries. Subqueries might include term queries (finding specific individual words in the search expression - for example hotel), phrase queries (finding multi-term phrases specified in quotation marks in the search expression - for example, "free parking"), and prefix queries (finding terms with a specified prefix - for example air*, which would match airway, air-conditioning, and airport).
2. Lexical analysis - The query terms are analyzed and refined based on linguistic rules. For example, text is converted to lower case and nonessential stopwords (such as "the", "a", "is", and so on) are removed. Then words are converted to their root form (for example, "comfortable" might be simplified to "comfort") and composite words are split into their constituent terms.
3. Document retrieval - The query terms are matched against the indexed terms, and the set of matching documents is identified.
4. Scoring - A relevance score is assigned to each result based on a term frequency/inverse document frequency (TF/IDF) calculation.

**Filtering results**: you can apply filters to queries in two ways:

- By including filter criteria in a simple search expression.
- By providing an OData filter expression as a $filter parameter with a full syntax search expression.

**Filtering with facets**: facets are a useful way to present users with filtering criteria based on field values in a result set. They work best when a field has a small number of discrete values that can be displayed as links or options in the user interface.

To use facets, you must specify facetable fields for which you want to retrieve the possible values in an initial query. For example, you could use the following parameters to return all of the possible values for the author field:

search=*
facet=author

The results from this query include a collection of discrete facet values that you can display in the user interface for the user to select. Then in a subsequent query, you can use the selected facet value to filter the results:

search=*
$filter=author eq 'selected-facet-value-here'

**Sorting results**: By default, results are sorted based on the relevancy score assigned by the query process, with the highest scoring matches listed first.
However, you can override this sort order by including an OData orderby parameter that specifies one or more sortable fields and a sort order (asc or desc).
For example, to sort the results so that the most recently modified documents are listed first, you could use the following parameter values:
search=*
$orderby=last_modified desc

**Persisting extracted information**: While the index might be considered the primary output from an indexing process, the enriched data it contains might also be useful in other ways. For example:

- Since the index is essentially a collection of JSON objects, each representing an indexed record, it might be useful to export the objects as JSON files for integration into a data orchestration process for extract, transform, and load (ETL) operations.
- You may want to normalize the index records into a relational schema of tables for analysis and reporting.
- Having extracted embedded images from documents during the indexing process, you might want to save those images as files.

Azure AI Search supports these scenarios by enabling you to define a knowledge store in the skillset that encapsulates your enrichment pipeline. The knowledge store consists of projections of the enriched data, which can be JSON objects, tables, or image files. When an indexer runs the pipeline to create or update an index, the projections are generated and persisted in the knowledge store.